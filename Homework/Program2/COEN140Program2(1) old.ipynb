{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# potentially do cross validation here, start without\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html is useful for this\n",
    "\n",
    "traindata = pd.read_csv(\n",
    "    filepath_or_buffer='train.dat', \n",
    "    header=None, \n",
    "    sep='\\t')\n",
    "\n",
    "traincategory = traindata.iloc[:, 0]\n",
    "traindata = traindata.iloc[:, 1]\n",
    "\n",
    "testdata = pd.read_csv(\n",
    "    filepath_or_buffer='test.dat', \n",
    "    header=None, \n",
    "    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "# try different methods of feature extraction\n",
    "# implement diminsionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of c-mers for the row\n",
    "# this grabs three letters at a time\n",
    "# cmer refers to a count of characters\n",
    "def cmer(row, c=3):\n",
    "  # Given a row and parameter c, return the vector of c-mers associated with the row\n",
    "\n",
    "  if len(row) < c:\n",
    "    return [row]\n",
    "  cmers = []\n",
    "  for i in range(len(row)-c+1):\n",
    "    cmers.append(row[i:(i+c)])\n",
    "  return cmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# not fully confident this still works as intended\n",
    "\n",
    "def build_matrix(data, num):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    mat = [cmer(row, num) for row in data]\n",
    "\n",
    "    nrows = len(mat)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in mat:\n",
    "        wordlist = [x[0] for x in d]\n",
    "        nnz += len(set(wordlist))\n",
    "        d = wordlist\n",
    "        for w in d: #can change here to differen cmer/wmer\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in mat:\n",
    "        listofwords = [x[0] for x in d]\n",
    "        cnt = Counter(listofwords) #same as above with cmer/wemer\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "\n",
    "        for j, k in enumerate(keys):\n",
    "            ind[j + n] = idx[k]\n",
    "            val[j + n] = cnt[k]\n",
    "\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinedData = np.append(traindata, testdata)\n",
    "# this is the sparse matrix of frequencies\n",
    "# change parameter to build_matrix to change length of cmers\n",
    "cmerMatrix = build_matrix(combinedData, 3)[0:1566, :]\n",
    "testMatrix = build_matrix(combinedData, 3)[1566:, :]\n",
    "# not sure if this needs to be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(cmerMatrix, traincategory)\n",
    "\n",
    "testPred = classifier.predict(testMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_file = open('output.dat', 'w+')\n",
    "pd.Series(testPred).to_csv(\"output.dat\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features from peptides -- can apply bag of words or kmers (words as letters of peptide)\n",
    "# do not need to implement any algorithm from scratch -- use any libraries\n",
    "# should use dimensionality reduction -- must show that we tried to use it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
