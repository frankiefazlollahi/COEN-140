{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP4LjpeFKakv"
   },
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "izIyA0mB9l2-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# potentially do cross validation here, start without\n",
    "\n",
    "traindata = pd.read_csv(\n",
    "    filepath_or_buffer='train.dat', \n",
    "    header=None, \n",
    "    sep='\\n')\n",
    "\n",
    "testdata = pd.read_csv(\n",
    "    filepath_or_buffer='test.dat', \n",
    "    header=None, \n",
    "    sep='\\n')\n",
    "\n",
    "# make all data lowercase\n",
    "traindata = traindata[0].apply(lambda x: x.lower()).to_numpy()\n",
    "testdata = testdata[0].apply(lambda x: x.lower()).to_numpy()\n",
    "\n",
    "# separate category and data, remove punctuation\n",
    "traincategory = []\n",
    "\n",
    "for row in range(len(traindata)):\n",
    "  traindata[row] = traindata[row].replace('-', ' ').translate(str.maketrans('', '', string.punctuation))\n",
    "  traincategory.append(traindata[row][0])\n",
    "  traindata[row] = traindata[row][2:]\n",
    "\n",
    "# remove punctuation\n",
    "for row in range(len(testdata)):\n",
    "  testdata[row] = testdata[row].replace('-', ' ').translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AhbpS1pt1d2c"
   },
   "outputs": [],
   "source": [
    "# create list of c-mers for the row\n",
    "# this grabs three letters at a time\n",
    "# cmer refers to a count of characters\n",
    "def cmer(row, c=3):\n",
    "  # Given a row and parameter c, return the vector of c-mers associated with the row\n",
    "\n",
    "  if len(row) < c:\n",
    "    return [row]\n",
    "  cmers = []\n",
    "  for i in range(len(row)-c+1):\n",
    "    cmers.append(row[i:(i+c)])\n",
    "  return cmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Q4sZVr_XB2jI"
   },
   "outputs": [],
   "source": [
    "def wmer(row, w=3):\n",
    "   # Given a row and parameter w, return the vector of w-mers associated with the row\n",
    "    row = row.split()\n",
    "    if len(row) < w:\n",
    "      return [row]\n",
    "    wmers = []\n",
    "    for i in range(len(row)-w+1):\n",
    "      wmers.append(row[i:(i+w)])\n",
    "    return wmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8Ct9lR2KYRK"
   },
   "source": [
    "Knn classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Q_2sCEFvJx6u"
   },
   "outputs": [],
   "source": [
    "# build a term frequency vector and append to dataframe\n",
    "def buildtfv(data, num):\n",
    "  # create large matrix of all c/wmers for each row\n",
    "  # to switch between cmer and wmer, change line below\n",
    "  mat = pd.Series([wmer(row, num) for row in data])\n",
    "\n",
    "  # #TFV = term frequency vector\n",
    "  # tfv = pd.DataFrame()\n",
    "  templist = []\n",
    "  for index, row in mat.iteritems():\n",
    "    unique, counts = np.unique(row, return_counts=True)\n",
    "    doc = pd.DataFrame([dict(zip(unique, counts))])\n",
    "    templist.append(doc)\n",
    "  tfv = pd.concat(templist, ignore_index=True)\n",
    "  # tfv = pd.DataFrame.from_dict(map(dict, templist))\n",
    "  return tfv.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iHk4hi3lKTjY"
   },
   "outputs": [],
   "source": [
    "# term frequency inverse document frequency\n",
    "def tfidf(tfv):\n",
    "  df = (tfv > 0).sum(axis = 0)\n",
    "  idf = np.log(len(tfv) / df)\n",
    "  tf_idf = tfv * idf\n",
    "  return tf_idf\n",
    "\n",
    "tfvpre = buildtfv(traindata, 1)\n",
    "tfvpost = tfidf(tfvpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TjmoUJtyZuL7"
   },
   "outputs": [],
   "source": [
    "def cosineSim(tf_idf, row):\n",
    "  a = tf_idf.iloc[row] # might need to be [row, :]\n",
    "  B = tf_idf\n",
    "  dot = a.multiply(B).sum(axis=1)\n",
    "\n",
    "  a_len = np.sqrt(a.multiply(a).sum())\n",
    "  b_len = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "  b_len\n",
    "\n",
    "  dot / (a_len * b_len)\n",
    "\n",
    "  cos_similarities = pd.DataFrame(dot / (a_len * b_len))[0]\n",
    "  most_similar = cos_similarities.sort_values(ascending=False)\n",
    "\n",
    "  return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UMaUx7kvaG43"
   },
   "outputs": [],
   "source": [
    "def get_train_neighbors(training_set, labels, k):\n",
    "  distances = pd.DataFrame(columns = ['train row', 'dist', 'category'])\n",
    "  row = []\n",
    "  distance = []\n",
    "  cate = []\n",
    "  for index in range(len(training_set)):\n",
    "    row.append(training_set[index])\n",
    "    distance.append(cosineSim(tfvpost, index)[:k])\n",
    "    cate.append(labels[index])\n",
    "\n",
    "  distances['train row'] = row\n",
    "  distances['dist'] = distance\n",
    "  distances['category'] = cate\n",
    "  return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "umdRh-ojkq8d"
   },
   "outputs": [],
   "source": [
    "def vote(neighbors):\n",
    "  #operates on one specific row\n",
    "  class_counter = Counter()\n",
    "  dist = neighbors['dist']\n",
    "  for i in range(len(dist)):\n",
    "    # need to check categories of each neighbor\n",
    "    class_counter[traincategory[dist.index[i]]] += 1\n",
    "  return class_counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tmHvNBsbuqYR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def trainaccuracy():\n",
    "  #access indices of each neighbor in print(cosineSim(tfvpost, index)[:k].index[1])\n",
    "  nearest = get_train_neighbors(traindata, traincategory, 5)\n",
    "\n",
    "  trainpredict = []\n",
    "  for rindex in range(len(nearest.values)):\n",
    "    trainpredict += vote(nearest.iloc[rindex])\n",
    "\n",
    "  return f1_score(traincategory, trainpredict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jQv_CYnOvGe8"
   },
   "outputs": [],
   "source": [
    "# print(trainaccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gYiViTIC8c-T"
   },
   "outputs": [],
   "source": [
    "def testpredictions():\n",
    "  nearestN = get_train_neighbors(testdata, traincategory, 5)\n",
    "\n",
    "  testpredict = []\n",
    "  for rindex in range(len(nearestN.values)):\n",
    "    testpredict += vote(nearestN.iloc[rindex])\n",
    "\n",
    "  return testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apqQ8PXSaJPi"
   },
   "outputs": [],
   "source": [
    "test_predictions_file = open('output.dat', 'w+')\n",
    "predicted_df = pd.Series(testpredictions())\n",
    "predicted_df.to_csv(\"output.dat\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "COEN 140 Program 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
