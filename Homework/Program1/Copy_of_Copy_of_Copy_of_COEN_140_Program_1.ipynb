{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP4LjpeFKakv"
   },
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izIyA0mB9l2-",
    "outputId": "c086f1a6-5e54-4116-a00a-2f7e3a998dad"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "traindata = pd.read_csv(\n",
    "    filepath_or_buffer='train.dat', \n",
    "    header=None, \n",
    "    sep='\\t')\n",
    "\n",
    "traincategory = traindata.iloc[:, 0]\n",
    "traindata = traindata.iloc[:, 1]\n",
    "\n",
    "testdata = pd.read_csv(\n",
    "    filepath_or_buffer='test.dat', \n",
    "    header=None, \n",
    "    sep='\\t')\n",
    "\n",
    "# make all data lowercase\n",
    "traindata = traindata.apply(lambda x: x.lower()).to_numpy()\n",
    "testdata = testdata[0].apply(lambda x: x.lower()).to_numpy()\n",
    "\n",
    "# remove punctuation\n",
    "for row in range(len(traindata)):\n",
    "  traindata[row] = traindata[row].replace('-', ' ').translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# remove punctuation\n",
    "for row in range(len(testdata)):\n",
    "  testdata[row] = testdata[row].replace('-', ' ').translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AhbpS1pt1d2c"
   },
   "outputs": [],
   "source": [
    "# create list of c-mers for the row\n",
    "# this grabs three letters at a time\n",
    "# cmer refers to a count of characters\n",
    "def cmer(row, c=3):\n",
    "  # Given a row and parameter c, return the vector of c-mers associated with the row\n",
    "\n",
    "  if len(row) < c:\n",
    "    return [row]\n",
    "  cmers = []\n",
    "  for i in range(len(row)-c+1):\n",
    "    cmers.append(row[i:(i+c)])\n",
    "  return cmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q4sZVr_XB2jI"
   },
   "outputs": [],
   "source": [
    "def wmer(row, w=3):\n",
    "   # Given a row and parameter w, return the vector of w-mers associated with the row\n",
    "    row = row.split()\n",
    "    if len(row) < w:\n",
    "      return [row]\n",
    "    wmers = []\n",
    "    for i in range(len(row)-w+1):\n",
    "      wmers.append(row[i:(i+w)])\n",
    "    return wmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q_2sCEFvJx6u"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "#idx is dictionary as inpuit\n",
    "def build_train_matrix(data, idx = {}):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    mat = [wmer(row, 1) for row in data]\n",
    "\n",
    "    nrows = len(mat)\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "\n",
    "    for d in mat:\n",
    "      wordlist = [x[0] for x in d]\n",
    "      nnz += len(set(wordlist))\n",
    "      d = wordlist\n",
    "      for w in d: #can change here to differen cmer/wmer\n",
    "          if w not in idx:\n",
    "            idx[w] = tid\n",
    "            tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in mat:\n",
    "        listofwords = [x[0] for x in d]\n",
    "        cnt = Counter(listofwords) #same as above with cmer/wemer\n",
    "        keys = list(k for k,_ in cnt.most_common() if k in idx)\n",
    "        l = len(keys)\n",
    "\n",
    "        for j, k in enumerate(keys):\n",
    "            ind[j + n] = idx[k]\n",
    "            val[j + n] = cnt[k]\n",
    "\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    return mat, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a2FnLiyokh8M"
   },
   "outputs": [],
   "source": [
    "def build_test_matrix(data, idx = {}):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    #mat = data\n",
    "    mat = [wmer(row, 1) for row in data]\n",
    "\n",
    "    nrows = len(mat)\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "\n",
    "    for d in mat:\n",
    "      nnz += len([x[0] for x in d if x[0] in idx])\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in mat:\n",
    "        listofwords = [x[0] for x in d]\n",
    "        cnt = Counter(listofwords) #same as above with cmer/wemer\n",
    "        keys = list(k for k,_ in cnt.most_common() if k in idx)\n",
    "        l = len(keys)\n",
    "\n",
    "        for j, k in enumerate(keys):\n",
    "            ind[j + n] = idx[k]\n",
    "            val[j + n] = cnt[k]\n",
    "\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iHk4hi3lKTjY"
   },
   "outputs": [],
   "source": [
    "# term frequency inverse document frequency\n",
    "def tfidf(tfv):\n",
    "  df = (tfv > 0).sum(axis = 0)\n",
    "  idf = np.log(tfv.getnnz() / df)\n",
    "  tf_idf = tfv * idf\n",
    "  return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8Ct9lR2KYRK"
   },
   "source": [
    "Knn classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x9WdDZ3Lmn9n"
   },
   "outputs": [],
   "source": [
    "traintfv, idx = build_train_matrix(traindata)\n",
    "testtfv = build_test_matrix(testdata, idx)\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "traintfv = csr_l2normalize(traintfv, copy=True)\n",
    "testtfv = csr_l2normalize(testtfv, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TjmoUJtyZuL7"
   },
   "outputs": [],
   "source": [
    "def cosineSim(item, tf_idf):\n",
    "  a = item\n",
    "  B = tf_idf\n",
    "\n",
    "  dot = a.multiply(B).sum()\n",
    "\n",
    "  a_len = np.sqrt(a.multiply(a).sum())\n",
    "  b_len = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "\n",
    "  cos_similarities = pd.DataFrame(dot / (a_len * b_len))[0]\n",
    "  cos_similarities = cos_similarities.sort_values(ascending=False)\n",
    "  return cos_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UMaUx7kvaG43"
   },
   "outputs": [],
   "source": [
    "def get_neighbors(dataset, traintfv, tfvtest, labels, k):\n",
    "  #tfv has potential neighbors -- for each row of dataset, find neighbors from tfv\n",
    "  distances = pd.DataFrame(columns = ['train row', 'dist'])\n",
    "  row = []\n",
    "  distance = []\n",
    "  #cate = []\n",
    "  for index in range(len(dataset)):\n",
    "    row.append(index)\n",
    "    sim = getneighbors(index, tfvtest)[0:k]\n",
    "    distance.append(sim)\n",
    "    #cate.append(labels[sim[0][0]]) #might be weird\n",
    "  distances['train row'] = row\n",
    "  distances['dist'] = distance\n",
    "  #distances['category'] = cate\n",
    "  return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "884L7zghmgJH"
   },
   "outputs": [],
   "source": [
    "def getneighbors(row, tfv):\n",
    "  x = tfv[row,:]\n",
    "  dots = x.dot(tfv.T)\n",
    "  dots[0, row] = -1 # invalidate self-similarity\n",
    "  sims = list(zip(dots.indices, dots.data))\n",
    "  sims.sort(key=lambda x: x[1], reverse=True)\n",
    "  return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C247p--cqWwo"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9018168d6509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainneighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraintfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesttfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraincategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-620c872ea1f9>\u001b[0m in \u001b[0;36mget_neighbors\u001b[1;34m(dataset, traintfv, tfvtest, labels, k)\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfvtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#cate.append(labels[sim[0][0]]) #might be weird\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainneighbors = get_neighbors(traindata, traintfv, testtfv, traincategory, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "umdRh-ojkq8d"
   },
   "outputs": [],
   "source": [
    "def vote(neighbors):\n",
    "  #operates on one specific row\n",
    "  class_counter = Counter()\n",
    "  dist = neighbors['dist']\n",
    "  for i in range(len(neighbors)):\n",
    "    # need to check categories of each neighbor\n",
    "    # add one at the class category col that corresponds to the category of the particular item in dist\n",
    "    class_counter[traincategory[dist[i][0]]] += 1\n",
    "  return class_counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tmHvNBsbuqYR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def trainaccuracy(nearest):\n",
    "\n",
    "  trainpredict = []\n",
    "  #for each row, use nearest neighbors to vote\n",
    "  for index in range(len(nearest.values)):\n",
    "    trainpredict.append(vote(nearest.iloc[index]))\n",
    "\n",
    "  return f1_score(traincategory, trainpredict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "jQv_CYnOvGe8",
    "outputId": "3e48b020-8166-4d53-ad44-2a2a265bde2a"
   },
   "outputs": [],
   "source": [
    "print(trainaccuracy(trainneighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdZOn7lGVLnL"
   },
   "outputs": [],
   "source": [
    "testneighbors = get_neighbors(testdata, traintfv, testtfv, traincategory, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYiViTIC8c-T"
   },
   "outputs": [],
   "source": [
    "def testpredictions(nearest):\n",
    "  testpredict = []\n",
    "  for rindex in range(len(nearest.values)):\n",
    "    print(nearest.iloc[rindex])\n",
    "    testpredict.append(vote(nearest.iloc[rindex]))\n",
    "\n",
    "  return testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hcNm0a5docSQ",
    "outputId": "f0355ecf-a240-4568-88bd-c9c595debc1a"
   },
   "outputs": [],
   "source": [
    "predicted_df = testpredictions(testneighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apqQ8PXSaJPi"
   },
   "outputs": [],
   "source": [
    "test_predictions_file = open('output.dat', 'w+')\n",
    "pd.Series(predicted_df).to_csv(\"output.dat\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Copy of Copy of COEN 140 Program 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
