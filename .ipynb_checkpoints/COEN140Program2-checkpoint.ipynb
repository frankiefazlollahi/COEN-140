{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# potentially do cross validation here, start without\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html is useful for this\n",
    "\n",
    "traindata = pd.read_csv(\n",
    "    filepath_or_buffer='train.dat', \n",
    "    header=None, \n",
    "    sep='\\t')\n",
    "\n",
    "traincategory = traindata.iloc[:, 0]\n",
    "traindata = traindata.iloc[:, 1]\n",
    "\n",
    "testdata = pd.read_csv(\n",
    "    filepath_or_buffer='test.dat', \n",
    "    header=None, \n",
    "    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "# try different methods of feature extraction\n",
    "# implement diminsionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of c-mers for the row\n",
    "# this grabs three letters at a time\n",
    "# cmer refers to a count of characters\n",
    "def cmer(row, c=3):\n",
    "  # Given a row and parameter c, return the vector of c-mers associated with the row\n",
    "\n",
    "  if len(row) < c:\n",
    "    return [row]\n",
    "  cmers = []\n",
    "  for i in range(len(row)-c+1):\n",
    "    cmers.append(row[i:(i+c)])\n",
    "  return cmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# not fully confident this still works as intended\n",
    "\n",
    "def build_matrix(data, num):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    mat = [cmer(row, num) for row in data]\n",
    "\n",
    "    nrows = len(mat)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in mat:\n",
    "        wordlist = [x[0] for x in d]\n",
    "        nnz += len(set(wordlist))\n",
    "        d = wordlist\n",
    "        for w in d: #can change here to differen cmer/wmer\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in mat:\n",
    "        listofwords = [x[0] for x in d]\n",
    "        cnt = Counter(listofwords) #same as above with cmer/wemer\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "\n",
    "        for j, k in enumerate(keys):\n",
    "            ind[j + n] = idx[k]\n",
    "            val[j + n] = cnt[k]\n",
    "\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinedData = np.append(traindata, testdata)\n",
    "# this is the sparse matrix of frequencies\n",
    "# change parameter to build_matrix to change length of cmers\n",
    "cmerMatrix = build_matrix(combinedData, 3)[0:1566, :]\n",
    "testMatrix = build_matrix(combinedData, 3)[1566:, :]\n",
    "# not sure if this needs to be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(matthews_corrcoef(traincategory, classifier.predict(cmerMatrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10000, random_state=0)\n",
    "classifier.fit(cmerMatrix, traincategory)\n",
    "\n",
    "testPred = classifier.predict(testMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_file = open('output.dat', 'w+')\n",
    "pd.Series(testPred).to_csv(\"output.dat\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should use dimensionality reduction -- must show that we tried to use it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
